{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Topic Modelling\n",
    "from corextopic import corextopic as ct\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Spam/Duplicate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments is: 7639\n",
      "Unique comments is: 7058\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/travel_comments.csv')\n",
    "print(f\"Total comments is: {len(df['Comment'])}\")\n",
    "print(f\"Unique comments is: {len(df['Comment'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Spam/Duplicates rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel Comments\n",
      "--------------------\n",
      "Before cleaning: 7639\n",
      "After cleaning: 7058\n"
     ]
    }
   ],
   "source": [
    "print('Travel Comments')\n",
    "print('-'*20)\n",
    "print(f\"Before cleaning: {len(df['Comment'])}\")\n",
    "\n",
    "df.drop_duplicates(subset=['Comment'], inplace=True)\n",
    "print(f\"After cleaning: {len(df['Comment'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Topic Modeling (CorEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the BOW model and all the unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('travel_comments.csv')\n",
    "\n",
    "def get_corex_vectorizer_vocab(df):\n",
    "    comments = []\n",
    "    for row in df['Comment']:\n",
    "        text_tokenize = word_tokenize(row)\n",
    "        text_lower = [w.lower() for w in text_tokenize]\n",
    "        text_words_only = [w for w in text_lower if re.search('^[a-z]+$',w)]\n",
    "        text_drop_2letters = [w for w in text_words_only if len(w) > 2]\n",
    "        text_joined = ' '.join(text_drop_2letters)\n",
    "        comments.append(text_joined)\n",
    "\n",
    "    df['Comment'] = comments\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "    vectorizer = vectorizer.fit(df['Comment'])\n",
    "    vecs = vectorizer.transform(df['Comment'])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "\n",
    "    return vecs, vocab\n",
    "\n",
    "vecs, vocab = get_corex_vectorizer_vocab(df=df)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating anchor words for CorEx model to guide words to fall into each guided topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_words = [\n",
    "   # Topic 1: GPE\n",
    "   ['singapore', 'malaysia', 'united states', 'india', 'indonesia', 'china', 'germany', 'hong kong', 'united kingdom', 'japan'],\n",
    "\n",
    "   # Topic 2: PERSON\n",
    "   ['rs lin', 'cov fox bbc', 'boba fett', 'allen chua', 'jesus', 'foreign workers', 'peter kroll', 'richard wong', 'lisa lim', 'pm lee'],\n",
    "\n",
    "   # Topic 3: ORG\n",
    "   ['moh', 'lah', 'cnn', 'cdc places', 'channel news asia', 'parliament', 'govt', 'sia', 'vote peoples action party', 'united states'],\n",
    "\n",
    "   # Topic 4: EVENT\n",
    "   ['covid', 'omicron', 'delta', 'social distancing', 'coronavirus', 'wuhan virus', 'safe distancing', 'covid variant', 'safeentry', 'circuit breaker'],\n",
    "   \n",
    "   # Topic 5: NORP\n",
    "   ['singaporeans', 'singaporean', 'johor', 'malaysian', 'chinese', 'indian', 'indonesian', 'european', 'asian', 'african']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CorEx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Anchor word not in word column labels provided to CorEx: rs lin\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: cov fox bbc\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: pm lee\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: channel news asia\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: vote peoples action party\n",
      "---Training of CorEx model completed ---\n",
      "Time taken: 139.0783007144928 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=5, seed=2)\n",
    "anchored_topic_model.fit(vecs, words=vocab, anchors=anchor_words, anchor_strength=6)\n",
    "\n",
    "print(\"---Training of CorEx model completed ---\")\n",
    "print(\"Time taken: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the top words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: singapore, malaysia, united states, india, germany, indonesia, hong kong, united kingdom, japan, china\n",
      "\n",
      "Topic #2: boba fett, allen chua, lisa lim, richard wong, jesus, foreign workers, boba, fett, wong, chua\n",
      "\n",
      "Topic #3: united states, govt, lah, states, united, moh, parliament, cnn, like, pore govt\n",
      "\n",
      "Topic #4: covid, omicron, delta, covid nineteen, nineteen, coronavirus, social distancing, covid cases, cases, safe distancing\n",
      "\n",
      "Topic #5: singaporeans, singaporean, johor, malaysian, chinese, asian, indian, european, african, indonesian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_corex_top_topic_words(model, n_words):\n",
    "    for n in range(n_words):\n",
    "        topic_words,_,_ = zip(*model.get_topics(topic = n))\n",
    "        print('Topic #{}: '.format(n+1) + ', '.join(topic_words))\n",
    "        print()\n",
    "print_corex_top_topic_words(model=anchored_topic_model, n_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the total correlation plot to check the correlation score for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total Correlation (nats)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def plot_corex_total_correlation(model):\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     plt.bar(range(model.tcs.shape[0]), model.tcs, color='#4e79a7', width=0.5)\n",
    "#     plt.xlabel('Topic', fontsize=16)\n",
    "#     plt.ylabel('Total Correlation (nats)', fontsize=16)\n",
    "# plot_corex_total_correlation(model=anchored_topic_model)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(anchored_topic_model.tcs.shape[0]), anchored_topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics for each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you have book the pcr test klia before arrival</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>access good information what investors need pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please include philippines also are super stre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omicron mutated times thats making existing va...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they are welcoming omicron into singapore when...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  topic_1  topic_2  \\\n",
       "0     you have book the pcr test klia before arrival      0.0      0.0   \n",
       "1  access good information what investors need pr...      0.0      0.0   \n",
       "2  please include philippines also are super stre...      0.0      0.0   \n",
       "3  omicron mutated times thats making existing va...      1.0      0.0   \n",
       "4  they are welcoming omicron into singapore when...      1.0      0.0   \n",
       "\n",
       "   topic_3  topic_4  topic_5  \n",
       "0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0  \n",
       "2      1.0      0.0      0.0  \n",
       "3      1.0      1.0      0.0  \n",
       "4      0.0      1.0      1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corex_label_topics(df, model, vecs, no_topics):\n",
    "    topic_df = pd.DataFrame(model.transform(vecs), columns=[\"topic_{}\".format(i+1) for i in range (no_topics)]).astype(float)\n",
    "    topic_df.index = df.index\n",
    "    df = pd.concat([df, topic_df], axis=1)\n",
    "    return df\n",
    "\n",
    "dominant_topics = corex_label_topics(df=df, model=anchored_topic_model, vecs=vecs, no_topics=5)\n",
    "\n",
    "dominant_topics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dominant_topics.to_csv('travel_topics_topic_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiments\n",
    "def get_sentiments(df):\n",
    "    df = vader_sentiment(df)\n",
    "    df = textblob_sentiment(df)\n",
    "    df['final_sentiment'] = df['Vader_compound_score'] + df['tb_polarity']\n",
    "\n",
    "    return df\n",
    "\n",
    "def vader_sentiment(df):\n",
    "    df['Vader_compound_score'] = df['Comment'].apply(lambda x: vader_compound_score(x))\n",
    "    return df\n",
    "\n",
    "def vader_compound_score(x):\n",
    "    vader_analyser = SentimentIntensityAnalyzer()\n",
    "    score = vader_analyser.polarity_scores(x)\n",
    "    return score['compound']\n",
    "\n",
    "def textblob_sentiment(df):\n",
    "    df['tb_polarity'] = df['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['tb_subjectivity'] = df['Comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get overview sentiments of comments for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1e1791eea73f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_sentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdominant_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_sentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'travel_topics_sentiment_labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2593dd04c33d>\u001b[0m in \u001b[0;36mget_sentiments\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sentiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvader_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextblob_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'final_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Vader_compound_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tb_polarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2593dd04c33d>\u001b[0m in \u001b[0;36mvader_sentiment\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvader_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Vader_compound_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader_compound_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2593dd04c33d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvader_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Vader_compound_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader_compound_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2593dd04c33d>\u001b[0m in \u001b[0;36mvader_compound_score\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvader_compound_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mvader_analyser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvader_analyser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    333\u001b[0m     ):\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36mmake_lex_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mlex_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlex_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_sentiments = get_sentiments(dominant_topics)\n",
    "# df_sentiments.to_csv('travel_topics_sentiment_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_comments_sentiments_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_pos = df_sentiments[df_sentiments['final_sentiment'] > 0]\n",
    "    df_sentiments_neu = df_sentiments[df_sentiments['final_sentiment'] == 0]\n",
    "    df_sentiments_neg = df_sentiments[df_sentiments['final_sentiment'] < 0]\n",
    "    \n",
    "    print(f\"There are {len(df_sentiments_pos)} positive comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(df_sentiments_neu)} neutral comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(df_sentiments_neg)} negative comments regarding topic {topic}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1229 positive comments regarding topic 1.\n",
      "There are 231 neutral comments regarding topic 1.\n",
      "There are 607 negative comments regarding topic 1.\n",
      "\n",
      "There are 677 positive comments regarding topic 2.\n",
      "There are 71 neutral comments regarding topic 2.\n",
      "There are 413 negative comments regarding topic 2.\n",
      "\n",
      "There are 723 positive comments regarding topic 3.\n",
      "There are 50 neutral comments regarding topic 3.\n",
      "There are 413 negative comments regarding topic 3.\n",
      "\n",
      "There are 733 positive comments regarding topic 4.\n",
      "There are 86 neutral comments regarding topic 4.\n",
      "There are 488 negative comments regarding topic 4.\n",
      "\n",
      "There are 716 positive comments regarding topic 5.\n",
      "There are 80 neutral comments regarding topic 5.\n",
      "There are 426 negative comments regarding topic 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_num_comments_sentiments_by_topic(topics_df=dominant_topics, topic=1)\n",
    "get_num_comments_sentiments_by_topic(topics_df=dominant_topics, topic=2)\n",
    "get_num_comments_sentiments_by_topic(topics_df=dominant_topics, topic=3)\n",
    "get_num_comments_sentiments_by_topic(topics_df=dominant_topics, topic=4)\n",
    "get_num_comments_sentiments_by_topic(topics_df=dominant_topics, topic=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top positive/neutral/negative comments under a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corex_top_positive_comments(topics_df, topic, n):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_pos = df_sentiments[df_sentiments['final_sentiment'] > 0]\n",
    "    # df_sentiments_pos.to_csv('df_sentiments_pos_topic_2.csv')\n",
    "\n",
    "    print(f\"There are {len(df_sentiments_pos)} positive comments regarding topic {topic}. {round(len(df_sentiments_pos)/len(df_sentiments), 2) * 100} % of the comments are positive for this topic.\\n\\n\")\n",
    "    \n",
    "    positive_comments = df_sentiments_pos.sort_values(by='final_sentiment', ascending=False)['Comment'].to_list()\n",
    "\n",
    "    print_top_comments(positive_comments, n)\n",
    "\n",
    "def get_corex_neutral_comments(topics_df, topic, n):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "\n",
    "    df_sentiments_neu = df_sentiments[df_sentiments['final_sentiment'] == 0]\n",
    "    # df_sentiments_neu_2.to_csv('df_sentiments_neu_topic_2.csv')\n",
    "\n",
    "    print(f\"There are {len(df_sentiments_neu)} neutral comments regarding topic {topic}. {round(len(df_sentiments_neu)/len(df_sentiments), 2) * 100} % of the comments are neutral for this topic.\\n\\n\")\n",
    "    \n",
    "    comments = df_sentiments_neu['Comment'].sample(n=n).to_list()\n",
    "\n",
    "    print_comments(comments, n)\n",
    "\n",
    "def get_corex_top_negative_comments(topics_df, topic, n):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_neg = df_sentiments[df_sentiments['final_sentiment'] < 0]\n",
    "    print(f\"There are {len(df_sentiments_neg)} negative comments regarding topic {topic}. {round(len(df_sentiments_neg)/len(df_sentiments), 2) * 100} % of the comments are negative for this topic.\\n\\n\")\n",
    "    \n",
    "    # comments = df_sentiments_neg['Comment'].sample(n=n).to_list()\n",
    "    negative_comments = df_sentiments_neg.sort_values(by='final_sentiment', ascending=True)['Comment'].to_list()\n",
    "\n",
    "    print_top_comments(negative_comments, n)\n",
    "\n",
    "def print_top_comments(comments, n):\n",
    "    for i in range(n):\n",
    "        print(f'Rank {i+1} comment:')\n",
    "        print(f'{comments[i]}')\n",
    "        print()\n",
    "\n",
    "def print_comments(comments, n):\n",
    "    for i in range(n):\n",
    "        print(f'Comment {i+1}')\n",
    "        print(f'{comments[i]}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1229 positive comments regarding topic 1. 59.0 % of the comments are positive for this topic.\n",
      "\n",
      "\n",
      "Rank 1 comment:\n",
      "the best thing that has ever happened united states allows united states wfh\n",
      "\n",
      "Rank 2 comment:\n",
      "mohd countries like congo indonesia philippines and malaysia who are corrupted are invited they got best democracy even united states where got riots the capitol building and donald trump can talk abt democracy\n",
      "\n",
      "Rank 3 comment:\n",
      "eight continues can cme here spread but got chance spread sounds fair singapore ministers fucxxx fuxx off don talk please tell united states something make united states happy\n",
      "\n",
      "Rank 4 comment:\n",
      "mark zuckerbird like someone bought umbrella but they need united states buy one protect them from the rain lol\n",
      "\n",
      "Rank 5 comment:\n",
      "thailand please can not eat all the delicious and cheap thai foods and see all their beautiful women lol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# positive comments for Topic 1\n",
    "get_corex_top_positive_comments(topics_df=dominant_topics, topic=1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 231 neutral comments regarding topic 1. 11.0 % of the comments are neutral for this topic.\n",
      "\n",
      "\n",
      "Comment 1\n",
      "monka eternal mine not valid singapore also got cases are also bringing cases over pcr tests dun work\n",
      "\n",
      "Comment 2\n",
      "not ask singapore ask the destination country\n",
      "\n",
      "Comment 3\n",
      "maybe you should move taiwan china\n",
      "\n",
      "Comment 4\n",
      "can take boat train malaysia need vaccinated travel lane\n",
      "\n",
      "Comment 5\n",
      "what about india\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# neutral comments for Topic 1\n",
    "get_corex_neutral_comments(topics_df=dominant_topics, topic=1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 607 negative comments regarding topic 1. 28.999999999999996 % of the comments are negative for this topic.\n",
      "\n",
      "\n",
      "Rank 1 comment:\n",
      "andylau google reporters without borders just deleted malaysia posts again one linked another explanation google contemptible and disgusting shitty paper\n",
      "\n",
      "Rank 2 comment:\n",
      "pap trying earn money and does not care about united states getting covid nineteen become sick die earning money out our misery and the corpse covid nineteen death absolutely inhumane\n",
      "\n",
      "Rank 3 comment:\n",
      "then why allow vaccinated travel lane into singapore all affected are coming singapore aggravate the situation disgusting\n",
      "\n",
      "Rank 4 comment:\n",
      "lin nothing with singapore how about germany then they are too having worst outbreak pandemic eversince vaccinated travel lane with singapore coincidence\n",
      "\n",
      "Rank 5 comment:\n",
      "schadenfreude that insulting singapore education system\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# negative comments for Topic 1\n",
    "get_corex_top_negative_comments(topics_df=dominant_topics, topic=1, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 733 positive comments regarding topic 4. 56.00000000000001 % of the comments are positive for this topic.\n",
      "\n",
      "\n",
      "Rank 1 comment:\n",
      "avijeet well tell that the doctor who presented his work and lab studies the covid gop conference the guy has been working covid since sars unbiaised has one the best credentials with regards this virus\n",
      "\n",
      "Rank 2 comment:\n",
      "angmo boss very happy more rrt thought won hope contract covid lorry soon\n",
      "\n",
      "Rank 3 comment:\n",
      "slumps perfect one hundred ranking covid recovery index source nikkei asia five nov\n",
      "\n",
      "Rank 4 comment:\n",
      "peter mish agree with you covid nineteen taught united states the importance having plan good all believe all can relate this\n",
      "\n",
      "Rank 5 comment:\n",
      "love singapore but not take medicines and injections and stay home but know the maks not anything but will wear one keep from getting fined malaysia groceries were delivered malaysia home before the call pandemic moved queretaro mexico eleven two two thousand and where safe beautiful and clean like singapore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# positive comments for Topic 4\n",
    "get_corex_top_positive_comments(topics_df=dominant_topics, topic=4, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 neutral comments regarding topic 4. 7.000000000000001 % of the comments are neutral for this topic.\n",
      "\n",
      "\n",
      "Comment 1\n",
      "omicron coming faster than getting the data\n",
      "\n",
      "Comment 2\n",
      "when this when everyone has contracted covid\n",
      "\n",
      "Comment 3\n",
      "what the daily infection rate pore from fourteen feb nineteen feb two thousand and\n",
      "\n",
      "Comment 4\n",
      "what happens when different countries have different definitions fully vaccinated you end the day still vaccinated travel lane but only another shape and form\n",
      "\n",
      "Comment 5\n",
      "onky living with covid ever changing restrictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# neutral comments for Topic 4\n",
    "get_corex_neutral_comments(topics_df=dominant_topics, topic=4, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 488 negative comments regarding topic 4. 37.0 % of the comments are negative for this topic.\n",
      "\n",
      "\n",
      "Rank 1 comment:\n",
      "andylau google reporters without borders just deleted malaysia posts again one linked another explanation google contemptible and disgusting shitty paper\n",
      "\n",
      "Rank 2 comment:\n",
      "pap trying earn money and does not care about united states getting covid nineteen become sick die earning money out our misery and the corpse covid nineteen death absolutely inhumane\n",
      "\n",
      "Rank 3 comment:\n",
      "hate covid hate covid hate covid hate covid hate covid\n",
      "\n",
      "Rank 4 comment:\n",
      "dislike comment section because has been infected with dangerous virus worst than covid\n",
      "\n",
      "Rank 5 comment:\n",
      "expect covid cases get worst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# negative comments for Topic 4\n",
    "get_corex_top_negative_comments(topics_df=dominant_topics, topic=4, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label emotions for each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(data):    \n",
    "\n",
    "    #remove html markup\n",
    "    data = re.sub(\"(<.*?>)\", \"\", data)\n",
    "\n",
    "    #remove urls\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    #remove hashtags and @names\n",
    "    data= re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data= re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "\n",
    "    #remove punctuation and non-ascii digits\n",
    "    data = re.sub(\"(\\\\W|\\\\d)\", \" \", data)\n",
    "    \n",
    "    #remove whitespace\n",
    "    data = data.strip()\n",
    "    \n",
    "    # tokenization with nltk\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    # stemming with nltk\n",
    "    porter = PorterStemmer()\n",
    "    stem_data = [porter.stem(word) for word in data]\n",
    "        \n",
    "    return stem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tl_ch\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\tl_ch\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\tl_ch\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\tl_ch\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you have book the pcr test klia before arrival</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>access good information what investors need pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please include philippines also are super stre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omicron mutated times thats making existing va...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they are welcoming omicron into singapore when...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[anger]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  topic_1  topic_2  \\\n",
       "0     you have book the pcr test klia before arrival      0.0      0.0   \n",
       "1  access good information what investors need pr...      0.0      0.0   \n",
       "2  please include philippines also are super stre...      0.0      0.0   \n",
       "3  omicron mutated times thats making existing va...      1.0      0.0   \n",
       "4  they are welcoming omicron into singapore when...      1.0      0.0   \n",
       "\n",
       "   topic_3  topic_4  topic_5    Emotion  \n",
       "0      0.0      0.0      0.0     [fear]  \n",
       "1      0.0      0.0      0.0  [neutral]  \n",
       "2      1.0      0.0      0.0      [joy]  \n",
       "3      1.0      1.0      0.0     [fear]  \n",
       "4      0.0      1.0      1.0    [anger]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emotions(df):\n",
    "    emotions_clf = pickle.load(open('Data/tfidf_svm.sav', 'rb'))\n",
    "    df['Emotion'] = df['Comment'].apply(lambda x: emotions_clf.predict([x]))\n",
    "    return df\n",
    "\n",
    "df_emot = emotions(dominant_topics)\n",
    "df_emot.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get overview emotions of comments for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_comments_by_emotions_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    comments_joy = df[df['Emotion'] == 'joy'].reset_index()\n",
    "    comments_sad = df[df['Emotion'] == 'sadness'].reset_index()\n",
    "    comments_anger = df[df['Emotion'] == 'anger'].reset_index()\n",
    "    comments_neu = df[df['Emotion'] == 'neutral'].reset_index()\n",
    "    comments_fear = df[df['Emotion'] == 'fear'].reset_index()\n",
    "\n",
    "    print(f\"There are {len(comments_joy)} joy comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(comments_sad)} sadness comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(comments_anger)} anger comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(comments_neu)} neutral comments regarding topic {topic}.\")\n",
    "    print(f\"There are {len(comments_fear)} fear comments regarding topic {topic}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 230 joy comments regarding topic 1.\n",
      "There are 245 sadness comments regarding topic 1.\n",
      "There are 260 anger comments regarding topic 1.\n",
      "There are 946 neutral comments regarding topic 1.\n",
      "There are 386 fear comments regarding topic 1.\n",
      "\n",
      "There are 151 joy comments regarding topic 2.\n",
      "There are 162 sadness comments regarding topic 2.\n",
      "There are 206 anger comments regarding topic 2.\n",
      "There are 358 neutral comments regarding topic 2.\n",
      "There are 284 fear comments regarding topic 2.\n",
      "\n",
      "There are 165 joy comments regarding topic 3.\n",
      "There are 148 sadness comments regarding topic 3.\n",
      "There are 178 anger comments regarding topic 3.\n",
      "There are 464 neutral comments regarding topic 3.\n",
      "There are 231 fear comments regarding topic 3.\n",
      "\n",
      "There are 161 joy comments regarding topic 4.\n",
      "There are 174 sadness comments regarding topic 4.\n",
      "There are 219 anger comments regarding topic 4.\n",
      "There are 448 neutral comments regarding topic 4.\n",
      "There are 305 fear comments regarding topic 4.\n",
      "\n",
      "There are 143 joy comments regarding topic 5.\n",
      "There are 162 sadness comments regarding topic 5.\n",
      "There are 213 anger comments regarding topic 5.\n",
      "There are 466 neutral comments regarding topic 5.\n",
      "There are 238 fear comments regarding topic 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1)\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2)\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3)\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4)\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top comments by emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_comments_by_emotions(df, emotion, n):\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    comments_emotion = df_sentiments[df_sentiments['Emotion'] == emotion].reset_index()\n",
    "\n",
    "    if emotion == 'joy':\n",
    "        comments = comments_emotion.sort_values(by='final_sentiment', ascending=False)['Comment'].to_list()\n",
    "    else:\n",
    "        comments = comments_emotion.sort_values(by='final_sentiment', ascending=True)['Comment'].to_list()\n",
    "    \n",
    "\n",
    "    print_top_comments(comments, n)\n",
    "\n",
    "def get_corex_top_comments_by_emotions(topics_df, topic, emotion, n):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "    \n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    comments_emotion = df_sentiments[df_sentiments['Emotion'] == emotion].reset_index()\n",
    "\n",
    "    if emotion == 'joy':\n",
    "        comments = comments_emotion.sort_values(by='final_sentiment', ascending=False)['Comment'].to_list()\n",
    "    else:\n",
    "        comments = comments_emotion.sort_values(by='final_sentiment', ascending=True)['Comment'].to_list()\n",
    "    \n",
    "\n",
    "    print_top_comments(comments, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 comment:\n",
      "avijeet well tell that the doctor who presented his work and lab studies the covid gop conference the guy has been working covid since sars unbiaised has one the best credentials with regards this virus\n",
      "\n",
      "Rank 2 comment:\n",
      "mohd countries like congo indonesia philippines and malaysia who are corrupted are invited they got best democracy even united states where got riots the capitol building and donald trump can talk abt democracy\n",
      "\n",
      "Rank 3 comment:\n",
      "eight continues can cme here spread but got chance spread sounds fair singapore ministers fucxxx fuxx off don talk please tell united states something make united states happy\n",
      "\n",
      "Rank 4 comment:\n",
      "mark zuckerbird like someone bought umbrella but they need united states buy one protect them from the rain lol\n",
      "\n",
      "Rank 5 comment:\n",
      "hope can visit batam soon beautiful adventure and looking forward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_top_comments_by_emotions(df=df_emot, emotion='joy', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 comment:\n",
      "pap trying earn money and does not care about united states getting covid nineteen become sick die earning money out our misery and the corpse covid nineteen death absolutely inhumane\n",
      "\n",
      "Rank 2 comment:\n",
      "such cold hearted measure for seniors who are not vaccinated they fell shall will regardless covid infection they might hesitate see doctor for the concern medical cost here you are suggesting them suffer and die alone home the worst case you regard them suicidal and neglect them\n",
      "\n",
      "Rank 3 comment:\n",
      "wrong the person died with omicron not omicron you need see past the word games they play the person was elderly cancer patient\n",
      "\n",
      "Rank 4 comment:\n",
      "only stupid people believe that living with covid nineteen can work live with covid nineteen means everyone will infected covid nineteen and get sick even die and have long term implication\n",
      "\n",
      "Rank 5 comment:\n",
      "hide malaysia test paper from malaysia mom when fail the test that one know lazy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_corex_top_comments_by_emotions(topics_df=df_emot, topic=4, emotion='sadness', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 comment:\n",
      "dislike comment section because has been infected with dangerous virus worst than covid\n",
      "\n",
      "Rank 2 comment:\n",
      "fiost the worst yet come and will made covid nineteen look like walk the park\n",
      "\n",
      "Rank 3 comment:\n",
      "rubarani yes but knowing the virus existed and gathering the thousands plain irresponsible what even worse knowing you have mysterious variant and still have the audacity travel out the country and spread the misery everywhere\n",
      "\n",
      "Rank 4 comment:\n",
      "english worst than covid nineteen impact\n",
      "\n",
      "Rank 5 comment:\n",
      "over fifty zero breakthrough cases sick covid arizona alone but\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_corex_top_comments_by_emotions(topics_df=df_emot, topic=4, emotion='fear', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 comment:\n",
      "avijeet well tell that the doctor who presented his work and lab studies the covid gop conference the guy has been working covid since sars unbiaised has one the best credentials with regards this virus\n",
      "\n",
      "Rank 2 comment:\n",
      "angmo boss very happy more rrt thought won hope contract covid lorry soon\n",
      "\n",
      "Rank 3 comment:\n",
      "peter mish agree with you covid nineteen taught united states the importance having plan good all believe all can relate this\n",
      "\n",
      "Rank 4 comment:\n",
      "now the spike protein has been improved and made one hundred more lethal knowing that the virus was never isolated and thus lab manufactured welcome covid two zero upgraded version\n",
      "\n",
      "Rank 5 comment:\n",
      "singapore welcome omicron would like sincerely welcome you our country here you are exempted from any taxes and are free roam you get what mean hope you enjoy your stay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_corex_top_comments_by_emotions(topics_df=df_emot, topic=4, emotion='joy', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 comment:\n",
      "andylau google reporters without borders just deleted malaysia posts again one linked another explanation google contemptible and disgusting shitty paper\n",
      "\n",
      "Rank 2 comment:\n",
      "hate covid hate covid hate covid hate covid hate covid\n",
      "\n",
      "Rank 3 comment:\n",
      "demonthatgotlordkeith you are totally lame why don you just come out and say you are anti vaccine anti mask and anti lockdown during the pandemic your are being cowardly that what your talking about just say\n",
      "\n",
      "Rank 4 comment:\n",
      "don hate covid understand that the virus not responsible for making everyone suffer\n",
      "\n",
      "Rank 5 comment:\n",
      "cases yesterday and yet today simplify rules for travelers open doors for covid nineteen come stupid repeat the mistake stupid believing peoples action party stupidest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_corex_top_comments_by_emotions(topics_df=df_emot, topic=4, emotion='anger', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2067 positive comments regarding topic 1.\n",
      "There are 1161 positive comments regarding topic 2.\n",
      "There are 1186 positive comments regarding topic 3.\n",
      "There are 1307 positive comments regarding topic 4.\n",
      "There are 1222 positive comments regarding topic 5.\n"
     ]
    }
   ],
   "source": [
    "def get_num_comments_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "    print(f\"There are {len(df)} positive comments regarding topic {topic}.\")\n",
    "\n",
    "get_num_comments_by_topic(topics_df=dominant_topics, topic=1)\n",
    "get_num_comments_by_topic(topics_df=dominant_topics, topic=2)\n",
    "get_num_comments_by_topic(topics_df=dominant_topics, topic=3)\n",
    "get_num_comments_by_topic(topics_df=dominant_topics, topic=4)\n",
    "get_num_comments_by_topic(topics_df=dominant_topics, topic=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1229 positive comments regarding topic 1.\n",
      "There are 607 negative comments regarding topic 1.\n",
      "There are 231 neutral comments regarding topic 1.\n",
      "\n",
      "There are 677 positive comments regarding topic 2.\n",
      "There are 413 negative comments regarding topic 2.\n",
      "There are 71 neutral comments regarding topic 2.\n",
      "\n",
      "There are 723 positive comments regarding topic 3.\n",
      "There are 413 negative comments regarding topic 3.\n",
      "There are 50 neutral comments regarding topic 3.\n",
      "\n",
      "There are 733 positive comments regarding topic 4.\n",
      "There are 488 negative comments regarding topic 4.\n",
      "There are 86 neutral comments regarding topic 4.\n",
      "\n",
      "There are 716 positive comments regarding topic 5.\n",
      "There are 426 negative comments regarding topic 5.\n",
      "There are 80 neutral comments regarding topic 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_num_positive_comments_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_pos = df_sentiments[df_sentiments['final_sentiment'] > 0]\n",
    "\n",
    "    print(f\"There are {len(df_sentiments_pos)} positive comments regarding topic {topic}.\")\n",
    "\n",
    "def get_num_negative_comments_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_pos = df_sentiments[df_sentiments['final_sentiment'] < 0]\n",
    "\n",
    "    print(f\"There are {len(df_sentiments_pos)} negative comments regarding topic {topic}.\")\n",
    "\n",
    "def get_num_neutral_comments_by_topic(topics_df, topic):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    df_sentiments = get_sentiments(df)\n",
    "\n",
    "    df_sentiments_pos = df_sentiments[df_sentiments['final_sentiment'] == 0]\n",
    "\n",
    "    print(f\"There are {len(df_sentiments_pos)} neutral comments regarding topic {topic}.\\n\")\n",
    "\n",
    "get_num_positive_comments_by_topic(topics_df=dominant_topics, topic=1)\n",
    "get_num_negative_comments_by_topic(topics_df=dominant_topics, topic=1)\n",
    "get_num_neutral_comments_by_topic(topics_df=dominant_topics, topic=1)\n",
    "\n",
    "get_num_positive_comments_by_topic(topics_df=dominant_topics, topic=2)\n",
    "get_num_negative_comments_by_topic(topics_df=dominant_topics, topic=2)\n",
    "get_num_neutral_comments_by_topic(topics_df=dominant_topics, topic=2)\n",
    "\n",
    "get_num_positive_comments_by_topic(topics_df=dominant_topics, topic=3)\n",
    "get_num_negative_comments_by_topic(topics_df=dominant_topics, topic=3)\n",
    "get_num_neutral_comments_by_topic(topics_df=dominant_topics, topic=3)\n",
    "\n",
    "get_num_positive_comments_by_topic(topics_df=dominant_topics, topic=4)\n",
    "get_num_negative_comments_by_topic(topics_df=dominant_topics, topic=4)\n",
    "get_num_neutral_comments_by_topic(topics_df=dominant_topics, topic=4)\n",
    "\n",
    "get_num_positive_comments_by_topic(topics_df=dominant_topics, topic=5)\n",
    "get_num_negative_comments_by_topic(topics_df=dominant_topics, topic=5)\n",
    "get_num_neutral_comments_by_topic(topics_df=dominant_topics, topic=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 230 joy comments regarding topic 1.\n",
      "There are 245 sadness comments regarding topic 1.\n",
      "There are 260 anger comments regarding topic 1.\n",
      "There are 946 neutral comments regarding topic 1.\n",
      "There are 386 fear comments regarding topic 1.\n",
      "\n",
      "There are 151 joy comments regarding topic 2.\n",
      "There are 162 sadness comments regarding topic 2.\n",
      "There are 206 anger comments regarding topic 2.\n",
      "There are 358 neutral comments regarding topic 2.\n",
      "There are 284 fear comments regarding topic 2.\n"
     ]
    }
   ],
   "source": [
    "def get_num_comments_by_emotions_by_topic(topics_df, topic, emotion):\n",
    "    df = topics_df[topics_df[f'topic_{topic}'] == 1.0].reset_index()\n",
    "\n",
    "    comments_emotion = df[df['Emotion'] == emotion].reset_index()\n",
    "    print(f\"There are {len(comments_emotion)} {emotion} comments regarding topic {topic}.\")\n",
    "\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1, emotion='joy')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1, emotion='sadness')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1, emotion='anger')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1, emotion='neutral')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=1, emotion='fear')\n",
    "print()\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2, emotion='joy')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2, emotion='sadness')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2, emotion='anger')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2, emotion='neutral')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=2, emotion='fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 165 joy comments regarding topic 3.\n",
      "There are 148 sadness comments regarding topic 3.\n",
      "There are 178 anger comments regarding topic 3.\n",
      "There are 464 neutral comments regarding topic 3.\n",
      "There are 231 fear comments regarding topic 3.\n",
      "\n",
      "There are 161 joy comments regarding topic 4.\n",
      "There are 174 sadness comments regarding topic 4.\n",
      "There are 219 anger comments regarding topic 4.\n",
      "There are 448 neutral comments regarding topic 4.\n",
      "There are 305 fear comments regarding topic 4.\n",
      "\n",
      "There are 143 joy comments regarding topic 5.\n",
      "There are 162 sadness comments regarding topic 5.\n",
      "There are 213 anger comments regarding topic 5.\n",
      "There are 466 neutral comments regarding topic 5.\n",
      "There are 238 fear comments regarding topic 5.\n"
     ]
    }
   ],
   "source": [
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3, emotion='joy')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3, emotion='sadness')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3, emotion='anger')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3, emotion='neutral')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=3, emotion='fear')\n",
    "print()\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4, emotion='joy')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4, emotion='sadness')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4, emotion='anger')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4, emotion='neutral')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=4, emotion='fear')\n",
    "print()\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5, emotion='joy')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5, emotion='sadness')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5, emotion='anger')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5, emotion='neutral')\n",
    "get_num_comments_by_emotions_by_topic(topics_df=df_emot, topic=5, emotion='fear')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca8176073e6bf12d9077ff009002fad0cb1dacc4343174635581a517ba1b213"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
